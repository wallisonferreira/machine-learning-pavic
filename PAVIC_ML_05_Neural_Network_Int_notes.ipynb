{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wallisonferreira/machine-learning-pavic/blob/main/PAVIC_ML_05_Neural_Network_Int_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNxQ27HCKBQb"
      },
      "source": [
        "Neste notebook, vamos codificar Redes Neurais de forma manual para tentar entender intuitivamente como elas são implementadas na prática."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scLxcHutKBQc"
      },
      "source": [
        "# Sumário"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zU22HLr9KBQd"
      },
      "source": [
        "- [Exemplo 1](#Exemplo-1)\n",
        "- [Exemplo 2](#Exemplo-2)\n",
        "- [Referências](#Referências)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMTztVO5KBQd"
      },
      "source": [
        "# Imports e Configurações"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BwJu6GFAKBQd"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0FmgOYpKBQe"
      },
      "source": [
        "# Exemplo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDtuh6i6KBQe"
      },
      "source": [
        "<img src='https://github.com/mafaldasalomao/pavic_treinamento_ml/blob/main/Machine_Learning/figures/backprop_example_1.png?raw=true'>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "-QGL_lG-KBQe"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x, derivative=False):\n",
        "  if derivative:\n",
        "    y = sigmoid(x)\n",
        "    return y * (1 - y)\n",
        "  return 1.0 /( 1.0 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PYqm6OsPKBQe"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0.05, 0.10]])\n",
        "y = np.array([[0.01, 0.99]])\n",
        "#pesos\n",
        "w1 = np.array([[0.15, 0.20], [0.25, 0.30]])\n",
        "#neste exemplo esta sendo usado um bias pra dois neuronios\n",
        "b1 = np.array([[0.35]]) # bias camada 1\n",
        "\n",
        "w2 = np.array([[0.40, 0.45], [0.50, 0.55]])\n",
        "b2 = np.array([[0.60]]) # bias camada 2\n",
        "\n",
        "learning_rate = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zLntV8AsUy0"
      },
      "source": [
        "## Forward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oeXTB17UsTEv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6862f05c-fafe-48ff-e2f1-82b9eb817ff8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.59326999, 0.59688438]]),\n",
              " array([[0.75136507, 0.77292847]]),\n",
              " 0.2983711087600027)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#ver 1 - slides 78-89\n",
        "#PUT YOUR CODE HERE\n",
        "#Forward\n",
        "for i in range(1):\n",
        "    #feed-forward\n",
        "\n",
        "    #1 camada\n",
        "    inp1 = np.dot(x, w1.T) + b1\n",
        "    h1 = sigmoid(inp1) #função de ativação\n",
        "    #2 camada\n",
        "    inp2 = np.dot(h1, w2.T) + b2\n",
        "    out = sigmoid(inp2)\n",
        "\n",
        "    cost = 0.5 * np.sum((y-out)**2) # MSE = 1/2N (yi-yî)²\n",
        "\n",
        "h1, out, cost\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32voAGlRtQTE"
      },
      "source": [
        "## BackProp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0tZUK3MUtTDG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66776cb7-fc35-4139-e3bb-5905071353d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.00043857 0.00087714]\n",
            " [0.00049771 0.00099543]]\n",
            "[[ 0.08216704  0.08266763]\n",
            " [-0.02260254 -0.02274024]]\n"
          ]
        }
      ],
      "source": [
        "#Put Your code Here\n",
        "# derivada em relação a saida predita\n",
        "dout = -(y - out) # derivada da função de custo\n",
        "\n",
        "#derivada em relação ao input2 (inp2) - 2 camada\n",
        "dinp2 = sigmoid(inp2, derivative=True) * dout\n",
        "\n",
        "# derivada de h1 em relação a dinp2\n",
        "dh1 = np.dot(dinp2, w2)\n",
        "\n",
        "# derivada em relação a w2\n",
        "dw2 = np.dot(dinp2.T, h1)\n",
        "\n",
        "# derivada em relação a b2\n",
        "db2 = 1.0 * dinp2.sum(axis=0, keepdims=True)\n",
        "\n",
        "# 1 camada\n",
        "dinp1 = sigmoid(inp1, derivative=True) * dh1\n",
        "\n",
        "dx = np.dot(dinp1, w1)\n",
        "dw1 = np.dot(dinp1.T, x)\n",
        "db1 = 1.0 * dinp1.sum(axis=0, keepdims=True)\n",
        "\n",
        "print(dw1)\n",
        "print(dw2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Atualização dos pesos\n",
        "w2 = w2 - learning_rate * dw2\n",
        "b2 = b2 - learning_rate * db2\n",
        "w1 = w1 - learning_rate * dw1\n",
        "b1 = b1 - learning_rate * db1\n",
        "\n",
        "w1, w2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJUO36vosKas",
        "outputId": "5ddb6646-0bac-4f5e-8d06-ba81ea023a63"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.14978072, 0.19956143],\n",
              "        [0.24975114, 0.29950229]]),\n",
              " array([[0.35891648, 0.40866619],\n",
              "        [0.51130127, 0.56137012]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rli2tpmZKBQe"
      },
      "source": [
        "# Exemplo 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GstWpODDF_Js"
      },
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:828/format:webp/1*fnU_3MGmFp0LBIzRPx42-w.png\"/>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-nS3dIDOF6H0"
      },
      "source": [
        "<img ulr=https://miro.medium.com/v2/resize:fit:828/format:webp/1*fnU_3MGmFp0LBIzRPx42-w.png>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "1CmSQgtfKBQe"
      },
      "outputs": [],
      "source": [
        "#PUT YOUR CODE HERE - cost functions\n",
        "\n",
        "def relu(x, derivative=False):\n",
        "    if derivative:\n",
        "        return np.where(x <= 0, 0, 1)\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "\n",
        "def linear(x, derivative=False):\n",
        "    return np.ones_like(x) if derivative else x\n",
        "\n",
        "# y_oh: y one-hot\n",
        "def softmax(x, y_oh=None, derivative=False):\n",
        "    if derivative:\n",
        "        y_pred = softmax(x)\n",
        "        k = np.nonzero(y_pred * y_oh) # [0.2, 0.1, 0.7] * [0,0,1]\n",
        "        pk = y_pred[k]\n",
        "        y_pred[k] = pk * (1.0 - pk)\n",
        "        return y_pred\n",
        "    exp = np.exp(x)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n",
        "def neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
        "    k = np.nonzero(y_pred * y_oh)\n",
        "    pk = y_pred[k]\n",
        "    if derivative:\n",
        "        y_pred[k] = (-1.0 / pk)\n",
        "        return y_pred\n",
        "    return np.mean(-np.log(pk))\n",
        "\n",
        "def softmax_neg_log_likelihood(y_oh, y_pred, derivative=False):\n",
        "    '''\n",
        "        y_oh: vetor one-hot\n",
        "        y_pred: vetor de valores esperados\n",
        "    '''\n",
        "    y_softmax = softmax(y_pred)\n",
        "    if derivative:\n",
        "        k = np.nonzero(y_pred * y_oh) # pega a saída desejada -> 1\n",
        "        dlog = neg_log_likelihood(y_oh, y_softmax, derivative=True) # -> derivada da neg\n",
        "        dsoftmax = softmax(y_pred, y_oh, derivative=True)\n",
        "        y_softmax[k] = dlog[k] * dsoftmax[k]\n",
        "        return y_softmax / y_softmax.shape[0] # média softmax\n",
        "    return neg_log_likelihood(y_oh, y_softmax)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NNQjLLA3KBQe"
      },
      "outputs": [],
      "source": [
        "x = np.array([[0.1, 0.2, 0.7]])\n",
        "y = np.array([[1, 0, 0]])\n",
        "w1 = np.array([[0.1, 0.2, 0.3], [0.3, 0.2, 0.7], [0.4, 0.3, 0.9]])\n",
        "b1 = np.ones((1,3))\n",
        "w2 = np.array([[0.2, 0.3, 0.5], [0.3, 0.5, 0.7], [0.6, 0.4, 0.8]])\n",
        "b2 = np.ones((1,3))\n",
        "w3 = np.array([[0.1, 0.4, 0.8], [0.3, 0.7, 0.2], [0.5, 0.2, 0.9]])\n",
        "b3 = np.ones((1,3))\n",
        "\n",
        "learning_rate = 0.01\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 301 épocas/atualizações de pesos\n",
        "for i in range(301):\n",
        "    #FEEDFORWARD\n",
        "    ## 1a camada\n",
        "    inp1 = np.dot(x, w1.T) + b1\n",
        "    h1 = relu(inp1) # h1 -> x da próxima camada\n",
        "\n",
        "    ## 2a camada\n",
        "    inp2 = np.dot(h1, w2.T) + b2\n",
        "    h2 = sigmoid(inp2)\n",
        "\n",
        "    ## 3a camada\n",
        "    inp3 = np.dot(h2, w3.T) + b3\n",
        "    out = linear(inp3)\n",
        "\n",
        "    # calcular custo\n",
        "    cost = softmax_neg_log_likelihood(y, out) # passamos o y esperado e o y obtido\n",
        "\n",
        "    #BACKPROPAGATION\n",
        "    dout = softmax_neg_log_likelihood(y, out, derivative=True) # a própria saída com a derivada\n",
        "\n",
        "    ## 3a camada\n",
        "    dinp3 = linear(inp3, derivative=True) * dout # multiplicado pela derivada de quem tá fora dout\n",
        "    dh2 = np.dot(dinp3, w3)\n",
        "    dw3 = np.dot(dinp3.T, h2)\n",
        "    db3 = 1.0 * dinp3.sum(axis=0, keepdims=True)\n",
        "\n",
        "    ## 2a camada\n",
        "    dinp2 = sigmoid(inp2, derivative=True) * dh2 # multiplicado pela derivada de quem tá fora dh2\n",
        "    dh1 = np.dot(dinp2, w2)\n",
        "    dw2 = np.dot(dinp2.T, h1)\n",
        "    db2 = 1.0 * dinp2.sum(axis=0, keepdims=True)\n",
        "\n",
        "    ## 1a camada\n",
        "    dinp1 = relu(inp1, derivative=True) * dh1 # multiplicado pela derivada de quem tá fora dh1\n",
        "    dx = np.dot(dinp1, w1)\n",
        "    dw1 = np.dot(dinp1.T, x)\n",
        "    db1 = 1.0 * dinp1.sum(axis=0, keepdims=True)\n",
        "\n",
        "    # atualização de pesos\n",
        "    w3 = w3 - learning_rate * dw3\n",
        "    b3 = b3 - learning_rate * db3\n",
        "\n",
        "    w2 = w2 - learning_rate * dw2\n",
        "    b2 = b2 - learning_rate * db2\n",
        "\n",
        "    w1 = w1 - learning_rate * dw1\n",
        "    b1 = b1 - learning_rate * db1\n",
        "\n",
        "    if cost % 30 == 0:\n",
        "        cost = softmax_neg_log_likelihood(y, out)\n",
        "        print(cost)\n",
        "\n",
        "for w in [w1, w2, w3]:\n",
        "    print('peso do ', w)"
      ],
      "metadata": {
        "id": "vq50Gl1KuGAw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b55e2ec-d639-4fb6-85cf-1bd245714c15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "peso do  [[0.10083595 0.2016719  0.30585165]\n",
            " [0.30086971 0.20173942 0.70608796]\n",
            " [0.40145052 0.30290104 0.91015363]]\n",
            "peso do  [[0.20544723 0.30673159 0.50749567]\n",
            " [0.30994562 0.5123005  0.71366784]\n",
            " [0.61065514 0.41317913 0.81464085]]\n",
            "peso do  [[ 0.66465527  0.98758148  1.39393956]\n",
            " [ 0.05020341  0.44006253 -0.06274803]\n",
            " [ 0.18514132 -0.12764401  0.56880846]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KH7pa_H1KBQf"
      },
      "source": [
        "# Referências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB6uGBsMKBQf"
      },
      "source": [
        "- [Neural Network from Scratch](https://beckernick.github.io/neural-network-scratch/)\n",
        "- [Backpropagation Algorithm](https://theclevermachine.wordpress.com/tag/backpropagation-algorithm/)\n",
        "- [Back-Propagation is very simple. Who made it Complicated ?](https://becominghuman.ai/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c)\n",
        "- [A Step by Step Backpropagation Example](https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/)\n",
        "- [Understanding softmax and the negative log-likelihood](https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}